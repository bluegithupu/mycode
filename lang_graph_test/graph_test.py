import os
import logging
import random
from typing import TypedDict, Annotated, Literal
from langchain_core.messages import AnyMessage, AIMessage, SystemMessage, HumanMessage
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END, add_messages

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['OPENAI_API_KEY'] = 'sk-cdjsim5KBne5thQJ2bF279E94fEa487aA347A7D85747Af10'
os.environ['OPENAI_API_BASE'] = 'https://api.rcouyi.com/v1'

# å®šä¹‰çŠ¶æ€
class ConversationState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    color: str

# å®šä¹‰è·¯ç”±èŠ‚ç‚¹
def router(state: ConversationState):
    logging.info("è¿›å…¥è·¯ç”±èŠ‚ç‚¹")
    prompt = """åˆ†æç”¨æˆ·è¾“å…¥çš„æƒ…ç»ªå€¾å‘ã€‚å¦‚æœæƒ…ç»ªåæ¶ˆæ,å›å¤"negative"ã€‚å¦‚æœæƒ…ç»ªåç§¯æ,å›å¤"positive"ã€‚åªå›å¤"negative"æˆ–"positive"ã€‚"""
    messages = state['messages'] + [SystemMessage(content=prompt)]
    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    response = model.invoke(messages)
    logging.info(f"è·¯ç”±èŠ‚ç‚¹åˆ†æç»“æœ: {response.content}")
    return {"messages": [response]}

# å®šä¹‰é»‘è‰²èŠ‚ç‚¹
def black_node(state: ConversationState):
    logging.info("è¿›å…¥é»‘è‰²èŠ‚ç‚¹")
    happy_emojis = ["ğŸ˜Š", "ğŸ˜„", "ğŸ˜ƒ", "ğŸ˜", "ğŸ˜†", "ğŸ˜", "ğŸ¥°", "ğŸ˜", "ğŸ¤—", "ğŸŒˆ", "ğŸŒ", "âœ¨", "ğŸ‰", "ğŸˆ"]
    chosen_emoji = random.choice(happy_emojis)
    response_content = f"çœ‹èµ·æ¥ä½ å¯èƒ½å¿ƒæƒ…ä¸å¤ªå¥½ã€‚å¸Œæœ›è¿™ä¸ªè¡¨æƒ…èƒ½è®©ä½ å¼€å¿ƒèµ·æ¥: {chosen_emoji}"
    logging.info(f"é»‘è‰²èŠ‚ç‚¹å›å¤: {response_content}")
    return {"messages": [AIMessage(content=response_content)], "color": "black"}

# å®šä¹‰ç™½è‰²èŠ‚ç‚¹
def white_node(state: ConversationState):
    logging.info("è¿›å…¥ç™½è‰²èŠ‚ç‚¹")
    prompt = """ç”¨æˆ·çš„æƒ…ç»ªåç§¯æã€‚ç»™å‡ºä¸€ä¸ªèµç¾å’Œé¼“åŠ±çš„å›å¤ã€‚"""
    messages = state['messages'] + [SystemMessage(content=prompt)]
    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    response = model.invoke(messages)
    logging.info(f"ç™½è‰²èŠ‚ç‚¹å›å¤: {response.content}")
    return {"messages": [response], "color": "white"}

# å®šä¹‰è·¯ç”±å†³ç­–å‡½æ•°
def route_decision(state: ConversationState) -> Literal["black", "white"]:
    last_message = state['messages'][-1].content.strip().lower()
    if last_message == "negative":
        logging.info("è·¯ç”±å†³ç­–: é€‰æ‹©é»‘è‰²èŠ‚ç‚¹")
        return "black"
    elif last_message == "positive":
        logging.info("è·¯ç”±å†³ç­–: é€‰æ‹©ç™½è‰²èŠ‚ç‚¹")
        return "white"
    else:
        logging.error(f"æ— æ•ˆçš„è·¯ç”±å“åº”: {last_message}")
        raise ValueError("Invalid router response")

# å®šä¹‰å›¾
workflow = StateGraph(ConversationState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("router", router)
workflow.add_node("black", black_node)
workflow.add_node("white", white_node)

# è®¾ç½®å…¥å£ç‚¹
workflow.set_entry_point("router")

# æ·»åŠ æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "router",
    route_decision,
    {
        "black": "black",
        "white": "white"
    }
)

# æ·»åŠ ç»“æŸè¾¹
workflow.add_edge("black", END)
workflow.add_edge("white", END)

# ç¼–è¯‘å›¾
app = workflow.compile()

# è¿è¡Œå¯¹è¯
async def run_conversation(user_input: str):
    logging.info(f"ç”¨æˆ·è¾“å…¥: {user_input}")
    messages = [HumanMessage(content=user_input)]
    async for s in app.astream({"messages": messages, "color": ""}, stream_mode="values"):
        if "color" in s and s["color"]:
            # logging.info(f"æœ€ç»ˆé¢œè‰²: {s['color']}")
            print(f"Final color: {s['color']}")
        for message in s["messages"]:
            if isinstance(message, AIMessage):
                # logging.info(f"AIå›å¤: {message.content}")
                print(f"AI: {message.content}")

# ä¸»å‡½æ•°
if __name__ == "__main__":
    import asyncio
    
    user_input = input("è¯·è¾“å…¥æ‚¨çš„æ¶ˆæ¯: ")
    asyncio.run(run_conversation(user_input))
